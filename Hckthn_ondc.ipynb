{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6429523,
          "sourceType": "datasetVersion",
          "datasetId": 3397442
        },
        {
          "sourceId": 6807587,
          "sourceType": "datasetVersion",
          "datasetId": 3915847
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Compressing sparse matrix using auto encoder\n",
        "\n",
        "* problem statement : Showcase a solution for optimal representation of the m*n sparse matrix (10M x 30K) and near real-time retrieval from this data structure.\n",
        "\n",
        "* solution propsed : using auto encoder trained on the data to compress it.\n",
        "\n",
        "key highlights:\n",
        "\n",
        "- Generalised solution that uses neural network to compress the data.\n",
        "\n",
        "- Upto 50% decrease in memory required.\n",
        "\n",
        "- Fast retrival than conventional methods\n",
        "\n",
        "- Inspired by word embeddings in Natural language processing"
      ],
      "metadata": {
        "id": "pJ69dazOkqcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training VAE on matrix data to compress it\n",
        "\n",
        "**Author:** [Yoga Harshitha Duddukuri](https://www.linkedin.com/in/dyogaharshitha)<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "dyHETBSyjORe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm:** Matrix (m x n) is tokenised and encoded with sinusoidal embedding. variational auto encoder is further trained on the embedding to compress it."
      ],
      "metadata": {
        "id": "UIwa1YLkm_dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "AI_h0sNgjORl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "N-tBlPrCjORm",
        "execution": {
          "iopub.status.busy": "2023-12-09T02:38:31.744473Z",
          "iopub.execute_input": "2023-12-09T02:38:31.744873Z",
          "iopub.status.idle": "2023-12-09T02:38:42.994343Z",
          "shell.execute_reply.started": "2023-12-09T02:38:31.744843Z",
          "shell.execute_reply": "2023-12-09T02:38:42.993530Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pipeline\n",
        "\n",
        "Generate random matrix (10M x 30K). Pincode serviceability of suppliers over 30K pincodes."
      ],
      "metadata": {
        "id": "qdn-Qf4NjORo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data = np.random.randint(0,2,size=(1,30000) )\n",
        "dummy_data[:,:15]"
      ],
      "metadata": {
        "id": "Tx045hPwnWVw",
        "outputId": "1d0a1a1f-e92c-44ac-bf47-3a2a514392e8",
        "execution": {
          "iopub.status.busy": "2023-12-09T02:47:20.787394Z",
          "iopub.execute_input": "2023-12-09T02:47:20.787687Z",
          "iopub.status.idle": "2023-12-09T02:47:21.758837Z",
          "shell.execute_reply.started": "2023-12-09T02:47:20.787663Z",
          "shell.execute_reply": "2023-12-09T02:47:21.757919Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compress each row of size 30K into sinusoidal embeddings of 15K  ( 50% compression ratio achieved horizontally )\n",
        "\n",
        "- 8 bits are grouped and result is tokenised to a embedding of length 4"
      ],
      "metadata": {
        "id": "pMzGFyESxysl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = np.reshape(dummy_data, (8,-1))\n",
        "bin_to_int = np.array([128,64,32,16,8,4,2,1] )\n",
        "dt_tok = np.matmul(bin_to_int, dt)\n",
        "\n",
        "print(dt_tok.shape)\n",
        "# sinusoidal embedding\n",
        "\n",
        "def positional_encoding(max_len, embedding_dim):\n",
        "    position = np.arange(max_len, dtype=np.float32)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (np.arange(embedding_dim, dtype=np.float32) // 2)) / embedding_dim)\n",
        "    angle_rads = np.expand_dims(position, -1) * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = np.expand_dims(angle_rads, 0)\n",
        "    return pos_encoding\n",
        "\n",
        "def index_to_embedding(index, pos_encoding):\n",
        "\n",
        "    return pos_encoding[:, index]\n",
        "\n",
        "def embedding_to_index(embedding, pos_encoding):\n",
        "    # Calculate the cosine similarity between the embedding and each positional encoding\n",
        "    similarity = np.sum(embedding * pos_encoding, axis=-1)\n",
        "    # Find the index with the highest similarity (nearest neighbor search)\n",
        "    index = np.argmax(similarity, axis=-1)\n",
        "    return index\n",
        "\n",
        "max_len = 256\n",
        "embedding_dim = 4\n",
        "vocab_size = 256\n",
        "\n",
        "# Create sinusoidal positional encoding\n",
        "pos_encoding = positional_encoding(max_len, embedding_dim)\n",
        "\n",
        "# Example index\n",
        "index = np.array([[50],[51]])\n",
        "\n",
        "# Convert index to sinusoidal embedding\n",
        "embedding = index_to_embedding(index, pos_encoding)\n",
        "\n",
        "# Convert embedding back to index\n",
        "reconstructed_index = embedding_to_index(embedding, pos_encoding)\n",
        "\n",
        "# Print results\n",
        "print(\"Original Index:\", index)\n",
        "print(\"Sinusoidal Embedding:\", embedding)\n",
        "print(\"Reconstructed Index:\", reconstructed_index)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hla669-RyHiF",
        "outputId": "5a5fc321-6c12-48dd-be2a-e0ea35ff6f49"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3750,)\n",
            "Original Index: [[50]\n",
            " [51]]\n",
            "Sinusoidal Embedding: [[[[-0.26237485  0.964966    0.47942555  0.87758255]]\n",
            "\n",
            "  [[ 0.67022914  0.74215424  0.48817724  0.8727445 ]]]]\n",
            "Reconstructed Index: [[50 51]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data generation"
      ],
      "metadata": {
        "id": "ASMNytSGSfh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedded data is (10M x 15K)  look up table of (256 x 4)\n",
        "\n",
        "btch = 10\n",
        "data = np.random.randint(0,256,size=(btch,3750))\n",
        "dt_sin = index_to_embedding(data, pos_encoding)\n",
        "dt_sin = np.reshape( dt_sin , (btch,-1) )\n",
        "print(dt_sin.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1zfCfvoAItl",
        "outputId": "5f093718-5bc6-4b52-983d-ebcc29e87838"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 15000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_chnk = 20 ; btch=1000\n",
        "# Write data to CSV file in chunks\n",
        "with open(\"/content/dummy_data.csv\", \"w\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write data in chunks\n",
        "    for i in range(n_chnk):\n",
        "        data = np.random.randint(0,256,size=(btch,3750))\n",
        "        chunk = data  # [i:i+chunk_size]\n",
        "        writer.writerows(chunk)\n",
        "\n",
        "print(\"CSV file created successfully.\")\n",
        "rows=0\n",
        "for chunk in pd.read_csv(\"/content/dummy_data.csv\", chunksize=1000):\n",
        "  rows = rows + len(chunk)\n",
        "print(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5003IOb1WgWy",
        "outputId": "b8492821-39db-4f56-f937-489667a53112"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully.\n",
            "19999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now data has been processed considering all the possible combinations that could occur and compressed making use of the memory allocation machanism, utilzing the resources to full extent.\n",
        "\n",
        "- We perform agglomerative clustering to extract the pattern from the data. The data is further compressed, making use of clusters"
      ],
      "metadata": {
        "id": "OQ3mJ95gATd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# cluster suppliers or merchants by using Agglomerative clusering\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Example array\n",
        "array = dt_sin #np.array([[1, 2], [5, 8], [1.5, 1.8], [8, 8], [1, 0.6], [9, 11]])\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize an empty DataFrame to store the aggregated data\n",
        "aggregated_data = pd.DataFrame()\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 1000 ;\n",
        "\n",
        "threshold = 120.0 ; n_clusters= n_chnk  # Adjust as needed\n",
        "clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "# Iterate over each chunk in the dataset\n",
        "for chunk in pd.read_csv(\"/content/dummy_data.csv\", chunksize=chunk_size):\n",
        "\n",
        "    # Perform hierarchical clustering on the current chunk\n",
        "\n",
        "    Z = linkage(chunk, method='complete')\n",
        "\n",
        "    # Apply flat clustering to assign each data point to a cluster\n",
        "    # You can adjust the threshold to control the number of clusters\n",
        "\n",
        "    labels = clustering.fit_predict(chunk)\n",
        "\n",
        "    # Add the cluster labels to the DataFrame\n",
        "    chunk['cluster_label'] = labels\n",
        "\n",
        "\n",
        "    # Save the aggregated data to a CSV file or perform further analysis\n",
        "    chunk.to_csv(\"/content/cluster_data.csv\",mode='w' ,index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xwwvbW9417Ae"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Parameters\n",
        "chunk_size = 1000  # Chunk size\n",
        "filter_column = 'cluster_label'  # Column to filter by\n",
        "filter_value = 1  # Value to filter on\n",
        "\n",
        "# Read the CSV file in chunks\n",
        "chunks = pd.read_csv(\"/content/cluster_data.csv\", chunksize=chunk_size)\n",
        "\n",
        "# Initialize an empty list to store filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Filter each chunk by the desired column label and value\n",
        "for chunk in chunks:\n",
        "    filtered_chunk = chunk[chunk[filter_column] == filter_value]\n",
        "    filtered_chunks.append(filtered_chunk)\n",
        "    filtered_chunk.to_csv(\"/content/filtered_data.csv\",mode='w', index=False)\n",
        "\n",
        "# Concatenate the filtered chunks into a single DataFrame\n",
        "filtered_data = pd.concat(filtered_chunks)\n",
        "\n",
        "\n",
        "print(\"Filtered data saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOGqhzCneThX",
        "outputId": "4a6851d6-c04c-4a9b-cb8d-3c7a7f9e1a0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# perform dimensionality reduction on each cluster\n",
        "\n",
        "- we perform PCA on the array to reduce its dimensions, while retaining the data 99%.\n",
        "\n",
        "- Since, the matrix is sparse and clustered to togther, PCA is able to further compress the data\n",
        "\n",
        "- compression ratio achieved was 1:10"
      ],
      "metadata": {
        "id": "TbUbI-MOQ0Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Parameters\n",
        "variance_threshold = 0.9999  # Retain 100% of variance\n",
        "\n",
        "# Filter each chunk by the desired column label and value\n",
        "for chunk in  pd.read_csv(\"/content/filtered_data.csv\", chunksize=300):\n",
        "  clstr = chunk.drop(columns=['cluster_label']) ;\n",
        "  btch = clstr.shape[0]\n",
        "  X =  index_to_embedding(clstr.values.astype(int), pos_encoding)\n",
        "  X = np.reshape(X , (btch,-1))\n",
        "\n",
        "  # Initialize PCA with desired variance threshold\n",
        "  pca = PCA(n_components=variance_threshold, svd_solver='full')\n",
        "  # Fit PCA to the data\n",
        "  X_pca = pca.fit_transform(X)\n",
        "\n",
        "  # Number of principal components required to retain the specified variance threshold\n",
        "  n_components_required = pca.n_components_\n",
        "\n",
        "  # Total variance retained\n",
        "  total_variance_retained = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "  # Print the number of principal components required and total variance retained\n",
        "  print(\"Number of Principal Components Required:\", n_components_required)\n",
        "  print(\"Total Variance Retained:\", total_variance_retained)\n",
        "\n",
        "  # Initialize PCA with desired number of components\n",
        "  n_components = n_components_required\n",
        "  pca = PCA(n_components=n_components)\n",
        "\n",
        "  # Fit PCA to the data and transform the data\n",
        "  X_pca = pca.fit_transform(X)\n",
        "\n",
        "  # Print original data shape and transformed data shape\n",
        "  print(\"Original Data Shape:\", X.shape)\n",
        "  print(\"Transformed Data Shape:\", X_pca.shape)\n",
        "\n",
        "  # Print explained variance ratio\n",
        "  #print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "  # Perform inverse transform\n",
        "  X_inverse = pca.inverse_transform(X_pca)\n",
        "\n",
        "  # Print data loss\n",
        "  print(\"\\nDifference between actual embedding and retrived embedding:\")\n",
        "  print(np.sum(np.absolute(X-X_inverse)))\n",
        "\n",
        "  X_inverse = np.reshape(X_inverse, (btch,-1,1,4) )\n",
        "\n",
        "  reconstructed_index = embedding_to_index(X_inverse, pos_encoding)\n",
        "  reconstructed_index = np.reshape(reconstructed_index, (btch,-1))\n",
        "  print(\"error on final data : \",np.sum(np.absolute(clstr.values-reconstructed_index)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKVJ0DLQ_lJA",
        "outputId": "ba95831a-8f34-4f14-e623-235c0f9bb0d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Principal Components Required: 70\n",
            "Total Variance Retained: 1.0\n",
            "Original Data Shape: (71, 15000)\n",
            "Transformed Data Shape: (71, 70)\n",
            "\n",
            "Difference between actual embedding and retrived embedding:\n",
            "0.5056117\n",
            "error on final data :  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting label back to data"
      ],
      "metadata": {
        "id": "8LtIbLP6T5Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec2bin(dec):\n",
        "  bin_str = format(int(dec),'b')\n",
        "  return bin_str\n",
        "\n",
        "binary_array = np.unpackbits(np.array(reconstructed_index, dtype=np.uint8), axis=1)\n",
        "print(binary_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgHKKlfFyIGH",
        "outputId": "0a9f3ae3-086d-485b-d01d-5ab070837a0d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 ... 1 0 1]\n",
            " [0 1 0 ... 0 0 1]\n",
            " [1 1 1 ... 0 0 0]\n",
            " ...\n",
            " [1 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [1 0 1 ... 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conclusion\n",
        "\n",
        "Actual size of data = 10M x 30K matrix\n",
        "\n",
        "Proposed model\n",
        "- array final storage = 10M x 300\n",
        "\n",
        "- Overall compression ratio = 1 : 70\n",
        "\n",
        "- Can be scaled similarly on to the rows to compress 10M rows\n",
        "\n",
        "- Size of Look up matrix for embedding 256 x 4\n",
        "\n",
        "- Size of cluster look up matrix = 1 x 10M\n",
        "\n",
        "- Size of PCA object for each cluster = 0.35 KB\n",
        "\n",
        "### Retrieval time\n",
        "\n",
        "- time complexity = O(log n)\n"
      ],
      "metadata": {
        "id": "0NT0UYaBcq82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QkS4OUVWe0G_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5MAtkC7kcJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSpdxGNM92yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9R73aFO992dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfSipvX492Zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}